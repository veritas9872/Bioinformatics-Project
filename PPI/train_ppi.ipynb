{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/veritas/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import flags\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from PPI.utils import load_edgelist_data, to_normalized_sparse_tensor, \\\n",
    "    split_graph_edges, get_roc_score, to_sparse_tensor\n",
    "from model.gcn import GCN\n",
    "\n",
    "# Set hyper-parameters.\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float(name='lr', default=0.001, help='learning rate', lower_bound=0.)\n",
    "flags.DEFINE_float(name='val_r', default=0.02, help='Validation set ratio', lower_bound=0., upper_bound=1.)\n",
    "flags.DEFINE_float(name='test_r', default=0.02, help='Test set ratio', lower_bound=0., upper_bound=1.)\n",
    "flags.DEFINE_integer(name='seed', default=9872, help='Seed for data split. Not the seed for model training.')\n",
    "flags.DEFINE_float(name='dropout', default=0.5, help='Dropout rate.', lower_bound=0, upper_bound=1)\n",
    "flags.DEFINE_bool(name='bias', default=True, help='Whether to use bias in GCN model.')\n",
    "flags.DEFINE_float(name='l2', default=0.01, help='L2 weight decay factor for weights and biases of the model.')\n",
    "flags.DEFINE_integer(name='h1', default=16, lower_bound=1,\n",
    "                     help='Number of hidden features for the first hidden layer of the GCN.')\n",
    "flags.DEFINE_integer(name='h2', default=32, lower_bound=1,\n",
    "                     help='Number of hidden features for the second hidden layer of the GCN.')\n",
    "flags.DEFINE_integer(name='epochs', default=30, help='Number of training epochs.', lower_bound=1)\n",
    "flags.DEFINE_string(name='logs', default='./logs',\n",
    "                    help='root directory for logs, checkpoints, and other records.')\n",
    "FLAGS.mark_as_parsed()  # This is necessary for using FLAGS in jupyter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# NOTE: Sparse dropout is not implemented properly for graph mode.\n",
    "\n",
    "data_path = '../data/yeast.edgelist'\n",
    "adj = load_edgelist_data(data_path)  # Get adjacency matrix in CSR format.\n",
    "\n",
    "# The adjacency matrix is symmetric but has non-zero elements on the diagonal.\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "adj_orig = adj - sp.diags(adj.diagonal())  # Removing diagonal elements.\n",
    "adj_orig.eliminate_zeros()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "adj_train, edges_train, edges_val, edges_val_fake, edges_test, edges_test_fake = \\\n",
    "    split_graph_edges(adj, val_ratio=FLAGS.val_r, test_ratio=FLAGS.test_r, seed=FLAGS.seed)\n",
    "\n",
    "adj_train_norm = to_normalized_sparse_tensor(adj_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = GCN(num_features=num_nodes, h1=FLAGS.h1, h2=FLAGS.h2, dropout=FLAGS.dropout, bias=FLAGS.bias)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=FLAGS.lr)\n",
    "log_dir = Path(FLAGS.logs)\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "log_dir /= f'log_{len(list(log_dir.iterdir())) + 1}'\n",
    "writer = tf.summary.create_file_writer(logdir=str(log_dir))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "adj_train_labels = adj_train + sp.identity(num_nodes, dtype=np.float32)\n",
    "adj_train_labels = tf.convert_to_tensor(adj_train_labels.todense(), dtype=tf.float32)\n",
    "\n",
    "one_hot = to_sparse_tensor(sp.identity(num_nodes, dtype=np.float32))\n",
    "norm = num_nodes ** 2 / (2 * (num_nodes ** 2 - num_edges))\n",
    "position_weight = (num_nodes ** 2 - num_edges) / num_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Beginning Training!\n",
      "Epoch 01 >>> Train Loss: 1.1542, Train ROC: 0.8167, Train AP: 0.9970, Val ROC: 0.8093, VAL AP: 0.8700\n",
      "Epoch 02 >>> Train Loss: 1.1376, Train ROC: 0.8432, Train AP: 0.9976, Val ROC: 0.8390, VAL AP: 0.8987\n",
      "Epoch 03 >>> Train Loss: 1.1219, Train ROC: 0.8556, Train AP: 0.9979, Val ROC: 0.8528, VAL AP: 0.9094\n",
      "Epoch 04 >>> Train Loss: 1.1071, Train ROC: 0.8616, Train AP: 0.9980, Val ROC: 0.8595, VAL AP: 0.9141\n",
      "Epoch 05 >>> Train Loss: 1.0932, Train ROC: 0.8648, Train AP: 0.9980, Val ROC: 0.8631, VAL AP: 0.9164\n",
      "Epoch 06 >>> Train Loss: 1.0802, Train ROC: 0.8667, Train AP: 0.9981, Val ROC: 0.8652, VAL AP: 0.9177\n",
      "Epoch 07 >>> Train Loss: 1.0680, Train ROC: 0.8678, Train AP: 0.9981, Val ROC: 0.8664, VAL AP: 0.9185\n",
      "Epoch 08 >>> Train Loss: 1.0567, Train ROC: 0.8684, Train AP: 0.9981, Val ROC: 0.8671, VAL AP: 0.9189\n",
      "Epoch 09 >>> Train Loss: 1.0461, Train ROC: 0.8688, Train AP: 0.9981, Val ROC: 0.8676, VAL AP: 0.9192\n",
      "Epoch 10 >>> Train Loss: 1.0363, Train ROC: 0.8690, Train AP: 0.9981, Val ROC: 0.8678, VAL AP: 0.9194\n",
      "Epoch 11 >>> Train Loss: 1.0273, Train ROC: 0.8691, Train AP: 0.9981, Val ROC: 0.8679, VAL AP: 0.9194\n",
      "Epoch 12 >>> Train Loss: 1.0188, Train ROC: 0.8691, Train AP: 0.9981, Val ROC: 0.8680, VAL AP: 0.9195\n",
      "Epoch 13 >>> Train Loss: 1.0110, Train ROC: 0.8692, Train AP: 0.9981, Val ROC: 0.8681, VAL AP: 0.9196\n",
      "Epoch 14 >>> Train Loss: 1.0037, Train ROC: 0.8692, Train AP: 0.9981, Val ROC: 0.8681, VAL AP: 0.9196\n",
      "Epoch 15 >>> Train Loss: 0.9969, Train ROC: 0.8693, Train AP: 0.9981, Val ROC: 0.8682, VAL AP: 0.9196\n",
      "Epoch 16 >>> Train Loss: 0.9907, Train ROC: 0.8693, Train AP: 0.9981, Val ROC: 0.8682, VAL AP: 0.9197\n",
      "Epoch 17 >>> Train Loss: 0.9849, Train ROC: 0.8693, Train AP: 0.9981, Val ROC: 0.8682, VAL AP: 0.9197\n",
      "Epoch 18 >>> Train Loss: 0.9795, Train ROC: 0.8693, Train AP: 0.9981, Val ROC: 0.8683, VAL AP: 0.9197\n",
      "Epoch 19 >>> Train Loss: 0.9744, Train ROC: 0.8693, Train AP: 0.9981, Val ROC: 0.8683, VAL AP: 0.9197\n",
      "Epoch 20 >>> Train Loss: 0.9697, Train ROC: 0.8694, Train AP: 0.9981, Val ROC: 0.8683, VAL AP: 0.9197\n",
      "Epoch 21 >>> Train Loss: 0.9653, Train ROC: 0.8694, Train AP: 0.9981, Val ROC: 0.8684, VAL AP: 0.9198\n",
      "Epoch 22 >>> Train Loss: 0.9614, Train ROC: 0.8694, Train AP: 0.9981, Val ROC: 0.8684, VAL AP: 0.9198\n",
      "Epoch 23 >>> Train Loss: 0.9573, Train ROC: 0.8694, Train AP: 0.9981, Val ROC: 0.8684, VAL AP: 0.9198\n",
      "Epoch 24 >>> Train Loss: 0.9535, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8684, VAL AP: 0.9198\n",
      "Epoch 25 >>> Train Loss: 0.9500, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9198\n",
      "Epoch 26 >>> Train Loss: 0.9467, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9198\n",
      "Epoch 27 >>> Train Loss: 0.9436, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9199\n",
      "Epoch 28 >>> Train Loss: 0.9404, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9199\n",
      "Epoch 29 >>> Train Loss: 0.9376, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9199\n",
      "Epoch 30 >>> Train Loss: 0.9348, Train ROC: 0.8695, Train AP: 0.9981, Val ROC: 0.8685, VAL AP: 0.9199\n",
      "\n",
      "Training is Finished!\n",
      "Test ROC: 0.8650, Test AP: 0.9177\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eeb5d39002254de6bd4449e349ba445d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Beginning Training!')\n",
    "with writer.as_default():\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(1, FLAGS.epochs + 1)):  # Only 1 training iteration per epoch.\n",
    "        tic = time()\n",
    "        inputs = [one_hot, adj_train_norm]\n",
    "\n",
    "        with tf.GradientTape() as tape:  # Begin gradient calculations from here.\n",
    "            outputs = model(inputs, training=True)\n",
    "            weight_decay = sum(tf.reduce_mean(tf.nn.l2_loss(w)) for w in model.weights)\n",
    "            loss = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "                labels=adj_train_labels, logits=outputs, pos_weight=position_weight))\n",
    "            loss += FLAGS.l2 * weight_decay  # Implementing weight decay for further stabilization of training.\n",
    "        variables = model.trainable_variables\n",
    "        gradients = tape.gradient(target=loss, sources=variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(gradients, variables))\n",
    "\n",
    "        adjacency_recon = model(inputs, training=False).numpy()  # Get results in evaluation mode.\n",
    "        train_roc, train_ap = get_roc_score(adjacency_recon, real_edges=edges_train, fake_edges=edges_test_fake)\n",
    "        val_roc, val_ap = get_roc_score(adjacency_recon, real_edges=edges_val, fake_edges=edges_val_fake)\n",
    "\n",
    "        tf.summary.scalar(name='Train/Loss', data=loss, step=epoch, description='Training loss for each epoch.')\n",
    "        tf.summary.scalar(name='Train/ROC', data=train_roc, step=epoch, description='ROC for training set.')\n",
    "        tf.summary.scalar(name='Train/AP', data=train_ap, step=epoch, description='Average Precision for training set.')\n",
    "        tf.summary.scalar(name='Val/ROC', data=val_roc, step=epoch, description='ROC for validation set.')\n",
    "        tf.summary.scalar(name='Val/AP', data=val_ap, step=epoch, description='Average Precision for validation set.')\n",
    "\n",
    "        toc = int(time() - tic)\n",
    "        print(f'Epoch {epoch:02d} >>> Train Loss: {loss:.4f}, Train ROC: {train_roc:.4f}, Train AP: {train_ap:.4f},'\n",
    "              f' Val ROC: {val_roc:.4f}, VAL AP: {val_ap:.4f}')\n",
    "\n",
    "    # After training loop is finished.\n",
    "    writer.flush()\n",
    "    print('Training is Finished!')\n",
    "    test_roc, test_ap = get_roc_score(adjacency_recon, real_edges=edges_test, fake_edges=edges_test_fake)\n",
    "    print(f'Test ROC: {test_roc:.4f}, Test AP: {test_ap:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
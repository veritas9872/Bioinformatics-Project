{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioInformatcis Project 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load Success!!\n",
      "Data split Success!!\n"
     ]
    }
   ],
   "source": [
    "adj = load_data()\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()\n",
    "\n",
    "# Featureless\n",
    "features = sparse2tensor(sp.identity(num_nodes))\n",
    "num_features = num_nodes\n",
    "features_nonzero = num_nodes\n",
    "\n",
    "print(\"Data load Success!!\")\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false, orig_matrix = split_data(adj)\n",
    "adj_train_norm = normalize_graph(adj_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GCN(num_features, features_nonzero, num_h1=32, num_h2=16, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse2tensor(adj_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gcn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.68107 val_roc= 0.76183 val_ap= 0.68914\n",
      "Epoch: 0002 train_loss= 0.66503 val_roc= 0.86738 val_ap= 0.85808\n",
      "Epoch: 0003 train_loss= 0.61654 val_roc= 0.87221 val_ap= 0.86111\n",
      "Epoch: 0004 train_loss= 0.70334 val_roc= 0.87312 val_ap= 0.86290\n",
      "Epoch: 0005 train_loss= 0.62391 val_roc= 0.87184 val_ap= 0.86088\n",
      "Epoch: 0006 train_loss= 0.61635 val_roc= 0.87112 val_ap= 0.85969\n",
      "Epoch: 0007 train_loss= 0.63869 val_roc= 0.87138 val_ap= 0.86153\n",
      "Epoch: 0008 train_loss= 0.64838 val_roc= 0.87213 val_ap= 0.86182\n",
      "Epoch: 0009 train_loss= 0.64750 val_roc= 0.87058 val_ap= 0.86055\n",
      "Epoch: 0010 train_loss= 0.63817 val_roc= 0.87080 val_ap= 0.86009\n",
      "Epoch: 0011 train_loss= 0.62100 val_roc= 0.87020 val_ap= 0.86070\n",
      "Epoch: 0012 train_loss= 0.60420 val_roc= 0.87033 val_ap= 0.86005\n",
      "Epoch: 0013 train_loss= 0.60128 val_roc= 0.87049 val_ap= 0.86020\n",
      "Epoch: 0014 train_loss= 0.61537 val_roc= 0.87059 val_ap= 0.86087\n",
      "Epoch: 0015 train_loss= 0.61557 val_roc= 0.86899 val_ap= 0.85840\n",
      "Epoch: 0016 train_loss= 0.60597 val_roc= 0.87033 val_ap= 0.85958\n",
      "Epoch: 0017 train_loss= 0.59502 val_roc= 0.86666 val_ap= 0.85623\n",
      "Epoch: 0018 train_loss= 0.59272 val_roc= 0.86340 val_ap= 0.85063\n",
      "Epoch: 0019 train_loss= 0.59867 val_roc= 0.85825 val_ap= 0.84148\n",
      "Epoch: 0020 train_loss= 0.60049 val_roc= 0.85600 val_ap= 0.83797\n",
      "Epoch: 0021 train_loss= 0.60048 val_roc= 0.85698 val_ap= 0.83879\n",
      "Epoch: 0022 train_loss= 0.59463 val_roc= 0.85632 val_ap= 0.83850\n",
      "Epoch: 0023 train_loss= 0.59050 val_roc= 0.85708 val_ap= 0.83765\n",
      "Epoch: 0024 train_loss= 0.58779 val_roc= 0.85867 val_ap= 0.84179\n",
      "Epoch: 0025 train_loss= 0.58944 val_roc= 0.86061 val_ap= 0.84592\n",
      "Epoch: 0026 train_loss= 0.59299 val_roc= 0.85988 val_ap= 0.84418\n",
      "Epoch: 0027 train_loss= 0.58965 val_roc= 0.85886 val_ap= 0.84466\n",
      "Epoch: 0028 train_loss= 0.58570 val_roc= 0.85794 val_ap= 0.84367\n",
      "Epoch: 0029 train_loss= 0.58426 val_roc= 0.85751 val_ap= 0.84262\n",
      "Epoch: 0030 train_loss= 0.58403 val_roc= 0.85666 val_ap= 0.83983\n",
      "Epoch: 0031 train_loss= 0.58395 val_roc= 0.85390 val_ap= 0.83699\n",
      "Epoch: 0032 train_loss= 0.58566 val_roc= 0.85261 val_ap= 0.83524\n",
      "Epoch: 0033 train_loss= 0.58267 val_roc= 0.85529 val_ap= 0.84139\n",
      "Epoch: 0034 train_loss= 0.58155 val_roc= 0.85489 val_ap= 0.83946\n",
      "Epoch: 0035 train_loss= 0.58086 val_roc= 0.85578 val_ap= 0.84407\n",
      "Epoch: 0036 train_loss= 0.58083 val_roc= 0.85690 val_ap= 0.84433\n",
      "Epoch: 0037 train_loss= 0.58190 val_roc= 0.85739 val_ap= 0.84782\n",
      "Epoch: 0038 train_loss= 0.58099 val_roc= 0.85606 val_ap= 0.84473\n",
      "Epoch: 0039 train_loss= 0.57918 val_roc= 0.85599 val_ap= 0.84668\n",
      "Epoch: 0040 train_loss= 0.58001 val_roc= 0.85128 val_ap= 0.83874\n",
      "Epoch: 0041 train_loss= 0.57812 val_roc= 0.85052 val_ap= 0.83928\n",
      "Epoch: 0042 train_loss= 0.57721 val_roc= 0.85068 val_ap= 0.84072\n",
      "Epoch: 0043 train_loss= 0.57657 val_roc= 0.85066 val_ap= 0.84301\n",
      "Epoch: 0044 train_loss= 0.57590 val_roc= 0.85041 val_ap= 0.84262\n",
      "Epoch: 0045 train_loss= 0.57473 val_roc= 0.84753 val_ap= 0.84069\n",
      "Epoch: 0046 train_loss= 0.57434 val_roc= 0.84534 val_ap= 0.83716\n",
      "Epoch: 0047 train_loss= 0.57353 val_roc= 0.84741 val_ap= 0.84030\n",
      "Epoch: 0048 train_loss= 0.57339 val_roc= 0.84393 val_ap= 0.83816\n",
      "Epoch: 0049 train_loss= 0.57265 val_roc= 0.84518 val_ap= 0.83996\n",
      "Epoch: 0050 train_loss= 0.57300 val_roc= 0.83674 val_ap= 0.83259\n",
      "Epoch: 0051 train_loss= 0.57158 val_roc= 0.84069 val_ap= 0.83724\n",
      "Epoch: 0052 train_loss= 0.57526 val_roc= 0.84875 val_ap= 0.84517\n",
      "Epoch: 0053 train_loss= 0.57142 val_roc= 0.84321 val_ap= 0.84091\n",
      "Epoch: 0054 train_loss= 0.57554 val_roc= 0.82754 val_ap= 0.82822\n",
      "Epoch: 0055 train_loss= 0.57054 val_roc= 0.83591 val_ap= 0.83533\n",
      "Epoch: 0056 train_loss= 0.57083 val_roc= 0.84216 val_ap= 0.84092\n",
      "Epoch: 0057 train_loss= 0.57347 val_roc= 0.84586 val_ap= 0.84427\n",
      "Epoch: 0058 train_loss= 0.56994 val_roc= 0.83789 val_ap= 0.83883\n",
      "Epoch: 0059 train_loss= 0.57208 val_roc= 0.83170 val_ap= 0.83451\n",
      "Epoch: 0060 train_loss= 0.57064 val_roc= 0.83544 val_ap= 0.83874\n",
      "Epoch: 0061 train_loss= 0.56972 val_roc= 0.83852 val_ap= 0.83982\n",
      "Epoch: 0062 train_loss= 0.57057 val_roc= 0.84416 val_ap= 0.84463\n",
      "Epoch: 0063 train_loss= 0.57003 val_roc= 0.84394 val_ap= 0.84391\n",
      "Epoch: 0064 train_loss= 0.56959 val_roc= 0.83896 val_ap= 0.84093\n",
      "Epoch: 0065 train_loss= 0.56968 val_roc= 0.83521 val_ap= 0.83722\n",
      "Epoch: 0066 train_loss= 0.56987 val_roc= 0.83651 val_ap= 0.83856\n",
      "Epoch: 0067 train_loss= 0.56961 val_roc= 0.83719 val_ap= 0.84079\n",
      "Epoch: 0068 train_loss= 0.56945 val_roc= 0.83799 val_ap= 0.84075\n",
      "Epoch: 0069 train_loss= 0.57026 val_roc= 0.84447 val_ap= 0.84413\n",
      "Epoch: 0070 train_loss= 0.56902 val_roc= 0.83745 val_ap= 0.83957\n",
      "Epoch: 0071 train_loss= 0.56909 val_roc= 0.83908 val_ap= 0.84024\n",
      "Epoch: 0072 train_loss= 0.56959 val_roc= 0.83609 val_ap= 0.83797\n",
      "Epoch: 0073 train_loss= 0.56937 val_roc= 0.83922 val_ap= 0.84090\n",
      "Epoch: 0074 train_loss= 0.56881 val_roc= 0.84080 val_ap= 0.84186\n",
      "Epoch: 0075 train_loss= 0.56920 val_roc= 0.84248 val_ap= 0.84281\n",
      "Epoch: 0076 train_loss= 0.56890 val_roc= 0.84294 val_ap= 0.84384\n",
      "Epoch: 0077 train_loss= 0.56860 val_roc= 0.84096 val_ap= 0.84217\n",
      "Epoch: 0078 train_loss= 0.56915 val_roc= 0.83791 val_ap= 0.84032\n",
      "Epoch: 0079 train_loss= 0.56888 val_roc= 0.83697 val_ap= 0.83918\n",
      "Epoch: 0080 train_loss= 0.56858 val_roc= 0.84269 val_ap= 0.84305\n",
      "Epoch: 0081 train_loss= 0.57072 val_roc= 0.84682 val_ap= 0.84645\n",
      "Epoch: 0082 train_loss= 0.56925 val_roc= 0.83675 val_ap= 0.83869\n",
      "Epoch: 0083 train_loss= 0.57000 val_roc= 0.83246 val_ap= 0.83657\n",
      "Epoch: 0084 train_loss= 0.56859 val_roc= 0.84091 val_ap= 0.84195\n",
      "Epoch: 0085 train_loss= 0.57079 val_roc= 0.84846 val_ap= 0.84739\n",
      "Epoch: 0086 train_loss= 0.56861 val_roc= 0.84214 val_ap= 0.84253\n",
      "Epoch: 0087 train_loss= 0.56915 val_roc= 0.83557 val_ap= 0.83865\n",
      "Epoch: 0088 train_loss= 0.56809 val_roc= 0.84106 val_ap= 0.84265\n",
      "Epoch: 0089 train_loss= 0.56814 val_roc= 0.84060 val_ap= 0.84254\n",
      "Epoch: 0090 train_loss= 0.56887 val_roc= 0.84401 val_ap= 0.84365\n",
      "Epoch: 0091 train_loss= 0.56814 val_roc= 0.84171 val_ap= 0.84274\n",
      "Epoch: 0092 train_loss= 0.56830 val_roc= 0.83759 val_ap= 0.83960\n",
      "Epoch: 0093 train_loss= 0.56807 val_roc= 0.84202 val_ap= 0.84227\n",
      "Epoch: 0094 train_loss= 0.56848 val_roc= 0.83487 val_ap= 0.83715\n",
      "Epoch: 0095 train_loss= 0.56876 val_roc= 0.84467 val_ap= 0.84473\n",
      "Epoch: 0096 train_loss= 0.56813 val_roc= 0.83864 val_ap= 0.84125\n",
      "Epoch: 0097 train_loss= 0.56795 val_roc= 0.83907 val_ap= 0.84007\n",
      "Epoch: 0098 train_loss= 0.56778 val_roc= 0.83895 val_ap= 0.84016\n",
      "Epoch: 0099 train_loss= 0.56772 val_roc= 0.84062 val_ap= 0.84132\n",
      "Epoch: 0100 train_loss= 0.56775 val_roc= 0.84191 val_ap= 0.84338\n",
      "Training is Finished\n",
      "Epoch: finished test_roc= 0.84191 test_ap= 0.84338\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "pos_weight = (num_nodes**2 - num_edges) / num_edges\n",
    "norm = num_nodes ** 2 / ((num_nodes**2 - num_edges) * 2)    \n",
    "    \n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    #print('Start of epoch %d' % (epoch,))\n",
    "    inputs = [features, adj_train_norm]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        recon = model(inputs, training=True)        \n",
    "        loss =  norm * tf.reduce_mean(\n",
    "                        tf.nn.weighted_cross_entropy_with_logits(labels=tf.sparse.to_dense(adj_label),\n",
    "                                                                logits=recon,\n",
    "                                                                pos_weight=pos_weight))\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    val_roc, val_ap = get_roc_score(recon, real_edges=val_edges, fake_edges=val_edges_false)\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "          \"train_loss=\", \"{:.5f}\".format(loss),\n",
    "          \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "print(\"Training is Finished\")\n",
    "    \n",
    "test_roc, test_ap = get_roc_score(recon, real_edges=test_edges, fake_edges=test_edges_false)  \n",
    "print(\"Epoch:\", 'finished', \n",
    "      \"test_roc=\", \"{:.5f}\".format(val_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(val_ap),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
